import torch
import torch.nn as nn
from models.base_model import BaseLanguageModel
from utils import decode_data


class LSTMLanguageModel(BaseLanguageModel):
    """
    An LSTM-based language model that predicts the next character in a sequence.

    Architecture:
        - Embedding layer converts character indices to dense vectors
        - LSTM layers process sequences to capture long-range dependencies
        - Linear layer projects LSTM output back to vocabulary size

    The model can maintain state between predictions, allowing it to learn
    longer-term patterns in the text compared to simpler models.
    """

    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int = 64,
        hidden_size: int = 128,
        num_layers: int = 2,
    ) -> None:
        """Initialize the LSTM model and its parameters."""
        super().__init__(vocab_size, model_name="lstm")

        self.embedding_dim = embedding_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Each character gets a vector of size embedding_dim
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # LSTM layers process the embedded sequences
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
        )
        # Final layer projects LSTM output back to vocabulary size
        self.fc = nn.Linear(hidden_size, vocab_size)

    def __repr__(self) -> str:
        """Return a string representation of the model."""
        output = (
            f"LSTMLanguageModel(\n"
            f"\tvocab_size={self.vocab_size},\n"
            f"\tembedding_dim={self.embedding_dim},\n"
            f"\thidden_size={self.hidden_size},\n"
            f"\tnum_layers={self.num_layers}\n"
            f")"
        )
        return output.expandtabs(4)

    def forward(
        self,
        idx: torch.Tensor,
        targets: torch.Tensor | None = None,
        hidden: tuple[torch.Tensor, torch.Tensor] | None = None,
    ) -> tuple[
        torch.Tensor, torch.Tensor | None, torch.Tensor | None, torch.Tensor | None
    ]:
        """
        Compute logits and loss for input indices and targets.
        Also returns the hidden state for use in generation.

        Args:
            idx: Input token indices of shape (B, T)
            targets: Target token indices of shape (B, T), optional
            hidden: LSTM hidden state tuple (h_n, c_n), optional

        Returns:
            tuple: (logits, loss, hidden) where:
                - logits: Model predictions of shape (B, T, vocab_size)
                - loss: Cross entropy loss if targets provided, None otherwise
                - hidden: Updated LSTM hidden state
        """
        # (B, T, embedding_dim): map indices to embeddings
        x = self.embedding(idx)
        # (B, T, hidden_size): process sequence with LSTM
        out, hidden = self.lstm(x, hidden)
        # (B, T, vocab_size): project to vocabulary size
        logits = self.fc(out)

        logits, loss = self.compute_loss(idx, logits, targets)

        return logits, loss, hidden

    @torch.no_grad()
    def generate(
        self,
        start_idx: int,
        itos: dict[int, str],
        max_new_tokens: int,
    ) -> str:
        """
        Generate new text by sampling from the model's predictions.
        Uses multinomial sampling to add randomness to the output.
        Starts from a seed index and generates max_new_tokens characters.
        Maintains the LSTM hidden state to capture context across generated tokens.
        Returns the decoded string generated by the model.

        """
        self.eval()
        idx = torch.tensor([[start_idx]], dtype=torch.long, device=self.device)
        generated = [start_idx]
        hidden = None

        for _ in range(max_new_tokens):
            # Get predictions and update hidden state for next step:
            logits, _, hidden = self(idx, hidden=hidden)
            next_idx = self.new_token(logits)
            # Prepare input for next iteration
            idx = next_idx
            generated.append(next_idx.item())

        return decode_data(generated, itos)
